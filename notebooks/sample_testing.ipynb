{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOqrVsKgd4I5OSiLJOTbDc2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Sample Testing**"],"metadata":{"id":"h6pEuDOCXp-M"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_fLkH3L9IcHf","executionInfo":{"status":"ok","timestamp":1739536180211,"user_tz":0,"elapsed":24318,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}},"outputId":"3f24080b-3aab-468a-cf7a-0a7628085792","collapsed":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pretty-midi\n","  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting muspy\n","  Downloading muspy-0.5.0-py3-none-any.whl.metadata (5.5 kB)\n","Collecting miditok\n","  Downloading miditok-3.0.4-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from pretty-midi) (1.26.4)\n","Collecting mido>=1.1.16 (from pretty-midi)\n","  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty-midi) (1.17.0)\n","Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.11/dist-packages (from muspy) (6.0.2)\n","Collecting bidict>=0.21 (from muspy)\n","  Downloading bidict-0.23.1-py3-none-any.whl.metadata (8.7 kB)\n","Requirement already satisfied: joblib>=0.15 in /usr/local/lib/python3.11/dist-packages (from muspy) (1.4.2)\n","Requirement already satisfied: matplotlib>=1.5 in /usr/local/lib/python3.11/dist-packages (from muspy) (3.10.0)\n","Collecting miditoolkit>=0.1 (from muspy)\n","  Downloading miditoolkit-1.0.1-py3-none-any.whl.metadata (4.9 kB)\n","Requirement already satisfied: music21>=6.0 in /usr/local/lib/python3.11/dist-packages (from muspy) (9.3.0)\n","Collecting pypianoroll>=1.0 (from muspy)\n","  Downloading pypianoroll-1.0.4-py3-none-any.whl.metadata (3.8 kB)\n","Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.11/dist-packages (from muspy) (2.32.3)\n","Requirement already satisfied: tqdm>=4.0 in /usr/local/lib/python3.11/dist-packages (from muspy) (4.67.1)\n","Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from miditok) (0.28.1)\n","Collecting symusic>=0.5.0 (from miditok)\n","  Downloading symusic-0.5.6-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n","Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from miditok) (0.21.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (3.17.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (24.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (4.12.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.5->muspy) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.5->muspy) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.5->muspy) (4.55.8)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.5->muspy) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.5->muspy) (11.1.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.5->muspy) (3.2.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.5->muspy) (2.8.2)\n","Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from music21>=6.0->muspy) (5.2.0)\n","Requirement already satisfied: jsonpickle in /usr/local/lib/python3.11/dist-packages (from music21>=6.0->muspy) (4.0.1)\n","Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from music21>=6.0->muspy) (10.6.0)\n","Requirement already satisfied: webcolors>=1.5 in /usr/local/lib/python3.11/dist-packages (from music21>=6.0->muspy) (24.11.1)\n","Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pypianoroll>=1.0->muspy) (1.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->muspy) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->muspy) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->muspy) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.0->muspy) (2025.1.31)\n","Collecting pySmartDL (from symusic>=0.5.0->miditok)\n","  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from symusic>=0.5.0->miditok) (4.3.6)\n","Downloading muspy-0.5.0-py3-none-any.whl (119 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.1/119.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading miditok-3.0.4-py3-none-any.whl (157 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading bidict-0.23.1-py3-none-any.whl (32 kB)\n","Downloading miditoolkit-1.0.1-py3-none-any.whl (24 kB)\n","Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypianoroll-1.0.4-py3-none-any.whl (26 kB)\n","Downloading symusic-0.5.6-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n","Building wheels for collected packages: pretty-midi\n","  Building wheel for pretty-midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pretty-midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592287 sha256=e7c63d1ba92fdcd30f753a228290800b7b811d595a3eaeae15fc53b1fcca5b1b\n","  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n","Successfully built pretty-midi\n","Installing collected packages: pySmartDL, symusic, mido, bidict, pretty-midi, pypianoroll, miditoolkit, muspy, miditok\n","Successfully installed bidict-0.23.1 miditok-3.0.4 miditoolkit-1.0.1 mido-1.3.3 muspy-0.5.0 pretty-midi-0.2.10 pySmartDL-1.3.4 pypianoroll-1.0.4 symusic-0.5.6\n"]}],"source":["!pip install pretty-midi muspy miditok"]},{"cell_type":"code","source":["import pandas as pd\n","import torch\n","import torch.nn as nn\n","import random\n","import muspy\n","import pretty_midi\n","import os\n","import pickle\n","import random\n","\n","from torch.utils.data import Dataset\n","from tqdm import tqdm\n","from transformers import GPT2LMHeadModel\n","from miditok import REMI, TokenizerConfig, TokSequence\n","\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j8fux4nFIfiP","executionInfo":{"status":"ok","timestamp":1739536233212,"user_tz":0,"elapsed":53001,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}},"outputId":"38d90daf-cf6b-484a-bca0-ac7ae33c59c8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["metadata = pd.read_csv(\"/content/drive/MyDrive/Piano generation/Project/MAESTRO dataset/maestro-v3.0.0.csv\")"],"metadata":{"id":"kEIg7bLGJOW6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this notebook we are going to generate some samples from every model, so that we can then test their performance. First, we are going to begin with the Many-to-One models. Let's load the necessary dataset:"],"metadata":{"id":"4xexmsV5Xvt5"}},{"cell_type":"code","source":["class MusicDatasetManyToOne(Dataset):\n","    def __init__(self, metadata, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.songs = []\n","\n","        for file in metadata[\"midi_filename\"]:\n","            file_path = os.path.join(\"/content/drive/MyDrive/Piano generation/Project/MAESTRO dataset/maestro-v3.0.0-midi/maestro-v3.0.0/\", file)\n","            muspy_file = muspy.read(file_path)\n","            self.songs.append(muspy_file)\n","\n","    def __len__(self):\n","        return len(self.songs)\n","\n","    def __getitem__(self, idx):\n","        pianoroll_song = self.songs[idx].to_pianoroll_representation()\n","        pianoroll_song = torch.tensor(pianoroll_song, dtype=torch.float32)\n","\n","        start_idx = random.randint(0, len(pianoroll_song) - self.seq_len - 1)\n","\n","        input_seq = pianoroll_song[start_idx:start_idx+self.seq_len]\n","\n","        target = pianoroll_song[start_idx+self.seq_len]\n","\n","        return input_seq, target"],"metadata":{"id":"W0mXYPAdJXC6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["music_dataset_test = torch.load(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/datasets/test/music_dataset_test_many_to_one_muspy.pt\", weights_only=False)"],"metadata":{"id":"bTX5lSIoJr7I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the function to generate music. It creates a for loop where the model generates the next tone, based on the previous 100 ones."],"metadata":{"id":"qkLaVnJoYC0U"}},{"cell_type":"code","source":["def generate_music(model, initial_sequence, seq_len=100, max_generate_len=5000):\n","    input_seq = initial_sequence\n","    generated_music = input_seq.squeeze(0)\n","    input_seq = input_seq.unsqueeze(0)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for _ in tqdm(range(max_generate_len)):\n","            output = model(input_seq)\n","            next_step = output\n","            generated_music = torch.cat((generated_music, next_step), dim=0)\n","            input_seq = generated_music[-seq_len:].unsqueeze(0)\n","\n","    return generated_music"],"metadata":{"id":"oXufWf4SJ-P0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the function to convert the pianoroll files into MIDI, using pretty_midi. It accepts `time_step` and `threshold` as hyperparameters, which are going to be made different for every model, based on subjective listening.\n","`time_step` defines the speed of the song, while `threshold` is the minimum value of the tone the model has to return in order for the tone to be played."],"metadata":{"id":"vOjUBG9AYQSl"}},{"cell_type":"code","source":["def pianoroll_to_midi(pianoroll, filepath, time_step=0.1, threshold=0.01):\n","    pianoroll = pianoroll.numpy()\n","    midi = pretty_midi.PrettyMIDI()\n","    instrument = pretty_midi.Instrument(program=0)\n","    current_pitches = {}\n","\n","    for i in range(pianoroll.shape[1]):\n","        current_pitches[i] = 0\n","\n","    for time in range(pianoroll.shape[0]):\n","        for pitch in range(pianoroll.shape[1]):\n","            if pianoroll[time, pitch] > threshold and current_pitches[i] == 0:\n","                current_pitches[pitch] = time\n","            if current_pitches[pitch] != 0 and pianoroll[time, pitch] <= threshold:\n","                note = pretty_midi.Note(velocity=100, pitch=pitch, start=current_pitches[pitch] * time_step, end=(time + 1) * time_step)\n","                instrument.notes.append(note)\n","                current_pitches[pitch] = 0\n","\n","    midi.instruments.append(instrument)\n","\n","    midi.write(filepath)"],"metadata":{"id":"CKRbqbaELdgS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's begin with the GRU model. Here we define it, initialize it and load the one after 150 epochs."],"metadata":{"id":"aVcmCW4eY7Zw"}},{"cell_type":"code","source":["class ManyToOneGRU(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers):\n","        super(ManyToOneGRU, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","\n","        out, _ = self.gru(x, h0)\n","        out = out[:, -1, :]\n","        out = self.fc(out)\n","\n","        return out"],"metadata":{"id":"vT-Q3q-IMroI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["many_to_one_gru = ManyToOneGRU(128, 256, 128, 2)\n","many_to_one_gru.load_state_dict(torch.load(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/saved_models/many_to_one_gru_fourth_try_epoch_150.pt\", map_location=torch.device(\"cpu\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8DMilaeoMvmD","executionInfo":{"status":"ok","timestamp":1739536262279,"user_tz":0,"elapsed":817,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}},"outputId":"a5725684-f7bb-40cf-9f66-33edd6d9eda8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-e6d4a01ac78a>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  many_to_one_gru.load_state_dict(torch.load(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/saved_models/many_to_one_gru_fourth_try_epoch_150.pt\", map_location=torch.device(\"cpu\")))\n"]},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["In this loop, we generate 10 songs, that go into the \"/saved_data/samples/many_to_one_gru/\" directory. The threshold value was changed in order to produce a somewhat acceptable result."],"metadata":{"id":"wPdNOdOqZDUR"}},{"cell_type":"code","source":["common_path = \"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/samples/many_to_one_gru\"\n","for i in range(10):\n","    initial_seq = music_dataset_test[random.randint(0, len(music_dataset_test) - 1)][0]\n","    generated_music = generate_music(many_to_one_gru, initial_seq, seq_len=100, max_generate_len=1000)\n","    pianoroll_to_midi(generated_music, os.path.join(common_path, f\"many_to_one_gru_generated_song_{i+1}.mid\"), threshold=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gTXkTcifMZUf","executionInfo":{"status":"ok","timestamp":1739536431566,"user_tz":0,"elapsed":169287,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}},"outputId":"8036b406-a3ce-415a-c645-11de08299771"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:15<00:00, 63.17it/s]\n","100%|██████████| 1000/1000 [00:17<00:00, 58.02it/s]\n","100%|██████████| 1000/1000 [00:16<00:00, 61.47it/s]\n","100%|██████████| 1000/1000 [00:15<00:00, 64.71it/s]\n","100%|██████████| 1000/1000 [00:15<00:00, 64.71it/s]\n","100%|██████████| 1000/1000 [00:16<00:00, 61.19it/s]\n","100%|██████████| 1000/1000 [00:16<00:00, 61.30it/s]\n","100%|██████████| 1000/1000 [00:15<00:00, 64.92it/s]\n","100%|██████████| 1000/1000 [00:15<00:00, 64.35it/s]\n","100%|██████████| 1000/1000 [00:15<00:00, 65.17it/s]\n"]}]},{"cell_type":"markdown","source":["Let's do exactly the same with the LSTM model."],"metadata":{"id":"KfX73GKTZS3F"}},{"cell_type":"code","source":["class ManyToOneLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, output_size, num_layers):\n","        super(ManyToOneLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n","        self.fc = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x):\n","        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n","\n","        out, (h_n, c_n) = self.lstm(x, (h0, c0))\n","        out = self.fc(h_n[-1])\n","\n","        return out"],"metadata":{"id":"Fk5x-9KSR-Io"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["many_to_one_lstm = ManyToOneLSTM(128, 256, 128, 2)\n","many_to_one_lstm.load_state_dict(torch.load(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/saved_models/many_to_one_lstm_third_try_epoch_150.pt\", map_location=torch.device(\"cpu\")))"],"metadata":{"id":"-CQRHvn4d2wQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Again, we generate 10 samples and save them in \"saved_data/samples/many_to_one_lstm/\"."],"metadata":{"id":"HB5J1iByZkNy"}},{"cell_type":"code","source":["common_path = \"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/samples/many_to_one_lstm\"\n","for i in range(10):\n","    initial_seq = music_dataset_test[random.randint(0, len(music_dataset_test) - 1)][0]\n","    generated_music = generate_music(many_to_one_lstm, initial_seq, seq_len=100, max_generate_len=1000)\n","    pianoroll_to_midi(generated_music, os.path.join(common_path, f\"many_to_one_lstm_generated_song_{i+1}.mid\"), threshold=0.1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KOOTr1aeASa","executionInfo":{"status":"ok","timestamp":1739536569235,"user_tz":0,"elapsed":136469,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}},"outputId":"bc3bef5b-47ac-4631-8aec-cbb24655fa15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1000/1000 [00:12<00:00, 80.45it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 80.41it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 79.52it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 79.67it/s]\n","100%|██████████| 1000/1000 [00:13<00:00, 72.72it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 78.39it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 78.44it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 77.78it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 79.27it/s]\n","100%|██████████| 1000/1000 [00:12<00:00, 79.50it/s]\n"]}]},{"cell_type":"markdown","source":["Now it's time for the Encoder-Decoder architecture. Let's again define and load the new dataset we have to use for this one."],"metadata":{"id":"6Z1cpagAZv-q"}},{"cell_type":"code","source":["class MusicDataset(Dataset):\n","    def __init__(self, metadata, seq_len):\n","        super().__init__()\n","        self.seq_len = seq_len\n","        self.songs = []\n","\n","        for file in metadata[\"midi_filename\"]:\n","            file_path = os.path.join(\"/content/drive/MyDrive/Piano generation/Project/MAESTRO dataset/maestro-v3.0.0-midi/maestro-v3.0.0/\", file)\n","            muspy_file = muspy.read(file_path)\n","            self.songs.append(muspy_file)\n","\n","    def __len__(self):\n","        return len(self.songs)\n","\n","    def __getitem__(self, idx):\n","        pianoroll_song = self.songs[idx].to_pianoroll_representation()\n","        pianoroll_song = torch.tensor(pianoroll_song, dtype=torch.float32)\n","\n","        start_idx = random.randint(0, len(pianoroll_song) - 2 * self.seq_len)\n","\n","        input_seq = pianoroll_song[start_idx:start_idx+self.seq_len]\n","\n","        target = pianoroll_song[start_idx+(self.seq_len // 2) : start_idx+self.seq_len+(self.seq_len // 2)]\n","\n","        return input_seq, target"],"metadata":{"id":"ASDBZas5eLJf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["music_dataset_test_many_to_many = torch.load(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/datasets/test/music_dataset_test_many_to_many_muspy.pt\")"],"metadata":{"id":"Zqcl6-VyfIT4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we initialize and load the trained model:"],"metadata":{"id":"7bCbUwxoZ70O"}},{"cell_type":"code","source":["class EncoderLSTM(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super(EncoderLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n","\n","    def forward(self, x):\n","        _, (h_n, c_n) = self.lstm(x)\n","\n","        h_n = h_n.view(self.num_layers, 2, x.size(0), self.hidden_size)\n","        c_n = c_n.view(self.num_layers, 2, x.size(0), self.hidden_size)\n","\n","        h_n = h_n.sum(dim=1)\n","        c_n = c_n.sum(dim=1)\n","\n","        return h_n, c_n"],"metadata":{"id":"WsTfQSs9fWPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DecoderLSTM(nn.Module):\n","    def __init__(self, output_size, hidden_size, num_layers):\n","        super(DecoderLSTM, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","        self.lstm = nn.LSTM(output_size, hidden_size, num_layers, batch_first=True)\n","        self.ff = nn.Linear(hidden_size, output_size)\n","\n","    def forward(self, x, h_n, c_n):\n","        output, (h_n, c_n) = self.lstm(x, (h_n, c_n))\n","        output = self.ff(output)\n","        return output, h_n, c_n"],"metadata":{"id":"ykCtSvf_fZ5K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Seq2SeqLSTM(nn.Module):\n","    def __init__(self, encoder, decoder):\n","        super(Seq2SeqLSTM, self).__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","\n","    def forward(self, x, target_seq_len, teacher_forcing_ratio=0.5):\n","\n","        batch_size = x.size(0)\n","        output_size = self.decoder.ff.out_features\n","        outputs = torch.zeros(batch_size, target_seq_len, output_size).to(x.device)\n","\n","        h_n, c_n = self.encoder(x)\n","\n","        decoder_input = torch.zeros(batch_size, 1, output_size).to(x.device)\n","\n","        for t in range(target_seq_len):\n","            output, h_n, c_n = self.decoder(decoder_input, h_n, c_n)\n","            outputs[:, t, :] = output.squeeze(1)\n","\n","            if torch.rand(1).item() < teacher_forcing_ratio:\n","                decoder_input = x[:, t, :].unsqueeze(1)\n","            else:\n","                decoder_input = output\n","\n","        return outputs"],"metadata":{"id":"Lk1hulrWfb5T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder = EncoderLSTM(input_size=128, hidden_size=1024, num_layers=2)\n","decoder = DecoderLSTM(output_size=128, hidden_size=1024, num_layers=2)\n","\n","encoder_decoder_lstm = Seq2SeqLSTM(encoder, decoder)\n","encoder_decoder_lstm.load_state_dict(torch.load(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/saved_models/encoder_decoder_lstm_epoch_120.pt\", map_location=torch.device(\"cpu\")))"],"metadata":{"id":"1MNkkug0fek6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This time, the generate_music function has to be different in order to work with the different nature pf the model."],"metadata":{"id":"TK11eQRVaAA_"}},{"cell_type":"code","source":["def generate_music_many_to_many(model, initial_sequence, seq_len=100, max_generate_len=5000):\n","    input_seq = initial_sequence\n","    generated_music = input_seq.squeeze(0)\n","    input_seq = input_seq.unsqueeze(0)\n","\n","    model.eval()\n","    with torch.no_grad():\n","        output = model(input_seq, max_generate_len, 0.0)\n","        return output.squeeze()"],"metadata":{"id":"kdzdUuKQgIQJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_path = \"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/samples/encoder_decoder_lstm\"\n","for i in tqdm(range(10)):\n","    initial_seq = music_dataset_test_many_to_many[random.randint(0, len(music_dataset_test_many_to_many) - 1)][0]\n","    generated_music = generate_music_many_to_many(encoder_decoder_lstm, initial_seq, seq_len=100, max_generate_len=500)\n","    pianoroll_to_midi(generated_music, os.path.join(common_path, f\"encoder_decoder_lstm_generated_song_{i+1}.mid\"), time_step=0.1, threshold=0.03)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xptQPdmwghv9","outputId":"1e2bcf4a-6f23-4884-ecb6-0a97c44932b2","executionInfo":{"status":"ok","timestamp":1739536714388,"user_tz":0,"elapsed":116436,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [01:56<00:00, 11.64s/it]\n"]}]},{"cell_type":"markdown","source":["At last, it's time to get samples from our GPT2 model. Let's again initialize the tokenizer and the model. From subjective listening to samples from both, I found the samples from the one after 10 epochs (the one that overfitted) to be better-sounding (although very far from good)."],"metadata":{"id":"-mA-_k41aSju"}},{"cell_type":"code","source":["tokenizer_config = TokenizerConfig(\n","    num_velocities=16,\n","    use_chords=True,\n","    use_programs=False,\n","    use_rests=True,\n","    use_tempos=True,\n","    use_time_signatures=False,\n","    use_sustain_pedals=True,\n",")\n","\n","tokenizer = REMI(tokenizer_config)"],"metadata":{"id":"TbdW-tZGmX8G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = len(tokenizer.vocab)\n","model = GPT2LMHeadModel.from_pretrained(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/saved_models/pretrained_gpt2_seventh_try_epoch_10.pt\")\n","model.resize_token_embeddings(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WYtdtlbL60FQ","executionInfo":{"status":"ok","timestamp":1739536719921,"user_tz":0,"elapsed":5550,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}},"outputId":"4a8ead87-6f76-401d-9728-5d99e8a32967"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Embedding(346, 768)"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["def generate_music_gpt2(model):\n","    input_ids = torch.tensor([[tokenizer.vocab['BOS_None']]])\n","\n","    with torch.no_grad():\n","        output = model.generate(\n","            input_ids=input_ids,\n","            max_length=512,\n","            temperature=1.0,\n","            top_k=50,\n","            top_p=0.95,\n","            do_sample=True,\n","            pad_token_id=tokenizer.vocab['PAD_None'],\n","            eos_token_id=tokenizer.vocab['EOS_None'],\n","            attention_mask=torch.ones_like(input_ids)\n","        )\n","\n","    return output[0].tolist()"],"metadata":{"id":"9ufOKB-w7aX1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def generated_to_midi(generated_ids):\n","    reverse_vocab = {v: k for k, v in tokenizer.vocab.items()}\n","    generated_tokens = [reverse_vocab.get(token_id, \"[UNK]\") for token_id in generated_ids]\n","\n","    tok_seq = TokSequence(tokens=generated_tokens, ids=generated_ids)\n","    tokenizer.complete_sequence(tok_seq)\n","\n","    midi = tokenizer([tok_seq])\n","    return midi"],"metadata":{"id":"FHRXDshgqzy3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in tqdm(range(10)):\n","    generated_ids = generate_music_gpt2(model)\n","    midi = generated_to_midi(generated_ids)\n","    midi.dump_midi(os.path.join(\"/content/drive/MyDrive/Piano generation/Project_draft/saved_data/samples/gpt2\", f\"gpt2_generated_song_{i+1}.mid\"))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vqsI5JVbT6G","executionInfo":{"status":"ok","timestamp":1739536957793,"user_tz":0,"elapsed":136203,"user":{"displayName":"Georgi Goranov","userId":"05647327236610301206"}},"outputId":"42cccce3-a17e-40f6-d696-50995968a308"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [02:15<00:00, 13.59s/it]\n"]}]}]}